# ElizaOS Contributor Analytics: Agent & Contributor Guide

This document provides a comprehensive overview of the ElizaOS Contributor Analytics project, designed to guide both AI agents and human contributors. It covers the project's purpose, architecture, key components, and development workflows.

**Agent Protocol**: When a plan is available (e.g., in the `plan/` directory), the agent MUST follow it step-by-step. After completing a task described in the plan, the agent MUST update the plan document to mark the task as complete (e.g., by checking a checkbox `[x]`). After a full phase of the plan is complete, the agent MUST commit the changes to the codebase before proceeding to the next phase. This ensures a clear record of progress and alignment with the project goals.

## 1. Project Overview

This project is a sophisticated analytics and visualization platform for GitHub repositories. Its primary goal is to track, analyze, score, and summarize contributor activity, providing valuable insights into the development process.

**Core Features:**

- **Data Ingestion**: Fetches comprehensive data (PRs, issues, commits, reviews, comments) from the GitHub API.
- **Scoring System**: A configurable algorithm that scores contributions based on type, complexity, and impact.
- **AI-Powered Summaries**: Generates daily, weekly, and monthly summaries of both individual contributor activity and overall project progress using LLMs.
- **Data Pipelines**: A robust, modular pipeline system for orchestrating data fetching, processing, and analysis.
- **Web Interface**: A Next.js application for visualizing data, leaderboards, and contributor profiles.
- **Automation**: GitHub Actions workflows for daily data processing and deployment.

**Tech Stack:**

- **Frontend**: Next.js, React, TypeScript, Tailwind CSS, shadcn/ui
- **Backend/Pipelines**: TypeScript, Bun
- **Database**: SQLite with Drizzle ORM
- **CI/CD**: GitHub Actions

---

## 2. System Architecture

The system is composed of three main parts: the data pipeline, the database, and the frontend application.

### 2.1. The Data Pipeline (`src/lib/pipelines/` & `cli/`)

This is the engine of the system. It's a TypeScript-based, modular system responsible for all data operations.

- **Entry Point**: The pipeline is executed via the CLI script at `cli/analyze-pipeline.ts`.
- **Orchestration**: It uses a functional composition model (`pipe`, `parallel`, `mapStep`) to build complex workflows from small, reusable steps. See `src/lib/pipelines/AGENTS.md` for a deep dive.
- **Configuration**: All pipeline behavior, including repository lists, scoring rules, and AI model settings, is defined in `config/pipeline.config.ts`.

The pipeline is divided into four main stages:

1.  **Ingest**: Fetches data from the GitHub API and stores it in the database.
2.  **Process**: Calculates contributor scores and expertise tags based on the raw data.
3.  **Export**: Generates aggregated JSON/MD files for static consumption by the frontend.
4.  **Summarize**: Uses an AI service (via OpenRouter) to generate summaries.

### 2.2. The Database (`src/lib/data/` & `drizzle/`)

A SQLite database serves as the single source of truth for all raw and processed data.

- **Schema**: Defined in `src/lib/data/schema.ts` using Drizzle ORM. It includes tables for users, repositories, all GitHub activities (PRs, issues, etc.), and processed data like scores and summaries.
- **Migrations**: Database schema changes are managed through migration files generated by Drizzle Kit, located in the `drizzle/` directory.
- **Data Integrity**: The schema is heavily indexed and uses foreign key relationships to ensure data consistency.

### 2.3. The Frontend (`src/app/` & `src/components/`)

A fully static Next.js 15 application hosted on GitHub Pages, designed to visualize the processed data. There is no backend server or API routes.

- **Static Site Generation (SSG)**: The site is generated at build time. Next.js Server Components are used to query the SQLite database directly during the build process, embedding the data into static HTML files.
- **Framework**: Uses the App Router for routing and Server Components for data fetching at build time.
- **Styling**: Tailwind CSS for utility-first styling, with `shadcn/ui` for pre-built, accessible components.

### 2.4. Authentication Worker (`auth-worker/`)

The `auth-worker/` directory contains a Cloudflare Worker responsible for handling the GitHub OAuth flow.

- **Purpose**: Its sole function is to securely exchange a temporary GitHub code for a user access token. This token is then passed back to the frontend.
- **Wallet Linking**: This mechanism is crucial for the wallet linking feature (`src/app/profile/edit/`). It allows contributors to authenticate with their GitHub account, which in turn grants the application permission to update their public profile `README.md` file with a hidden comment containing their wallet addresses.
- **See also**: `src/lib/walletLinking/**` for the core logic of parsing and generating the wallet data comment.

---

## 3. Automation & CI/CD (`.github/`)

Automation is a cornerstone of this project, ensuring data is always fresh and the application is consistently deployed.

### 3.1. GitHub Actions Workflows

- **`run-pipelines.yml`**: The main data processing workflow.
  - **Schedule**: Runs daily at 23:00 UTC.
  - **Process**: Executes the full `ingest -> process -> export -> summarize` pipeline.
  - **Data Management**: Uses a dedicated `_data` branch to store the generated data and SQLite database dump, keeping the `main` branch clean.
- **`deploy.yml`**: Handles the build and deployment to GitHub Pages.
  - **Trigger**: Runs after a successful pipeline run or on a push to `main`.
  - **Data Restoration**: Before building the Next.js app, it checks out the `_data` branch to ensure the latest data is included in the static site generation.

### 3.2. Custom Actions

The automation relies on two key custom GitHub Actions located in the `.github/actions/` directory:

- **`pipeline-data`**: Manages the lifecycle of the `_data` branch. It uses Git worktrees to check out the data branch, sync data between it and the main workspace, and commit updates.
- **`restore-db`**: Handles the serialization and deserialization of the SQLite database. It dumps the database into a version-controllable text format and can restore it, ensuring the schema is correctly migrated before loading the data.

### 3.3. Data Branch Strategy

- **`main` branch**: Contains the application code.
- **`_data` branch**: Contains all generated data, including the SQLite database dump.
- **Benefit**: This separation prevents the Git history of the application code from being cluttered with frequent data updates, simplifying code reviews and version control.

---

## 4. Developer Tooling

### 4.1. Data Sync Utility (`cli/data-sync.ts`)

To facilitate local development, the project includes a powerful data synchronization utility. This CLI script allows developers to pull the latest data directly from the production `_data` branch into their local environment.

- **Purpose**: It eliminates the need for developers to run the time-consuming `ingest` and `summarize` pipelines locally. Instead, they can get a complete, up-to-date dataset with a single command.
- **Mechanism**: The script mirrors the logic used in the GitHub Actions workflows. It fetches the `_data` branch, creates a temporary worktree, copies all the generated JSON/MD files, and restores the SQLite database from its version-controlled dump.
- **Usage**:

  ```bash
  # Sync data from the upstream repository
  bun run data:sync

  # See all options
  bun run data:sync --help
  ```

--

# Typescript Rules

- Dont cast to `any` type when you have a type error, instead think about the underlying issue causing the error and if/how you can update the related code to respect full typesafety.
- Avoid manual type signatures on functions and vars whenever possible, always prefer to use inference.
- Before declaring new Interfaces, search the codebase and types files to see if there's existing type definitions / zod schemas you can use
- avoid adding comments to code that is self explanatory
- For testing, use bun.sh test runner, and import from bun:test (https://bun.sh/docs/cli/test)
